{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpQFDIFL1sbIjayCFeAzMc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatenchtioui/NLP/blob/main/nlp_tp2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nettoyage des donnéées"
      ],
      "metadata": {
        "id": "O4BSqy7wcpK3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZgwoW5xXZOq"
      },
      "outputs": [],
      "source": [
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_Sentence(phrase):\n",
        "  sentence_w_punt=\"\".join([i.lower() for i in phrase if i not in string.punctuation])\n",
        "  sentence_w_num=\"\".join(i for i in sentence_w_punt if not i.isdigit())\n",
        "  return sentence_w_num"
      ],
      "metadata": {
        "id": "Cr9g5cEcZuyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase=\"faten chtioui @goole . com : c' est email  \"\n",
        "phrase_nor= preprocess_Sentence(phrase)\n",
        "print('phrase de base;'+phrase)\n",
        "print('phrase nettoyé:'+phrase_nor)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J_HBJk_bQDo",
        "outputId": "812e9ee3-f546-40e0-a852-618e86ee130b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "phrase de base;faten chtioui @goole . com : c' est email  \n",
            "phrase nettoyé:faten chtioui goole  com  c est email  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalisation"
      ],
      "metadata": {
        "id": "2Mt65YzddRa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B2FN2vp8duao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3RrWrt6b6hu",
        "outputId": "2216ba21-70e6-4ddd-af42-8572d2f18543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## télecharge les modéle francais"
      ],
      "metadata": {
        "id": "gQKk6YSFdxm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EnmvXX-dc7u",
        "outputId": "a82b40fd-c4a2-4353-d688-c0335e3b2455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-02 06:32:59.970067: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-02 06:33:01.166932: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting fr-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.6.0/fr_core_news_sm-3.6.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import Spacy et charge le modéle francais"
      ],
      "metadata": {
        "id": "c1TWK8zRd7NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"fr_core_news_sm\")"
      ],
      "metadata": {
        "id": "Q4k41jKAdrlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## une phrase exemple"
      ],
      "metadata": {
        "id": "SVYCBJbUehPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test =\" Il ya eu une coupure de réseau à Marseille. La panne a effecté 300.00 utilisateurs,\""
      ],
      "metadata": {
        "id": "zzsip_kcebHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = \"Il y a eu une coupure de réseau à Marseille\""
      ],
      "metadata": {
        "id": "ClTwTF34e4V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenisation"
      ],
      "metadata": {
        "id": "dOJqMH3jfDgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenisation par phrase"
      ],
      "metadata": {
        "id": "xHTVqq0bfJtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def return_token_sent(sentence):\n",
        "  # Tokeniser la phrase\n",
        "  doc = nlp(sentence)\n",
        "  # return le texte de chaque phrase\n",
        "  return [x.text for x in doc.sents]"
      ],
      "metadata": {
        "id": "JhxmIc9rfCvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result= return_token_sent(test)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es1rbAt0fxi6",
        "outputId": "54e7bdc3-f52a-403e-cf07-a3b5dcc5a45f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " 'Il ya eu une coupure de réseau à Marseille.',\n",
              " 'La panne a effecté 300.00 utilisateurs,']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## la tokenisation par mots"
      ],
      "metadata": {
        "id": "_MABXwbcgWBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def return_token(sentence):\n",
        "  # Tokeniser la phrase\n",
        "  doc = nlp(sentence)\n",
        "  # return le texte de chaque phrase\n",
        "  return [x.text for x in doc]"
      ],
      "metadata": {
        "id": "0ikUPvBggDOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "return_token(result[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Waljtjjogy3u",
        "outputId": "aa5668f1-9560-4703-f2ca-7e864875ba36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Il', 'ya', 'eu', 'une', 'coupure', 'de', 'réseau', 'à', 'Marseille', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### enleve les mmots les plus fréquent"
      ],
      "metadata": {
        "id": "Y5joUfmZizr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "O3NzwwbYg6jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopWordss = set(stopwords.words('french'))\n",
        "stopWordss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBAG6Ef7jgu2",
        "outputId": "d3c2238c-024c-49e0-eb51-4008625e8867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ai',\n",
              " 'aie',\n",
              " 'aient',\n",
              " 'aies',\n",
              " 'ait',\n",
              " 'as',\n",
              " 'au',\n",
              " 'aura',\n",
              " 'aurai',\n",
              " 'auraient',\n",
              " 'aurais',\n",
              " 'aurait',\n",
              " 'auras',\n",
              " 'aurez',\n",
              " 'auriez',\n",
              " 'aurions',\n",
              " 'aurons',\n",
              " 'auront',\n",
              " 'aux',\n",
              " 'avaient',\n",
              " 'avais',\n",
              " 'avait',\n",
              " 'avec',\n",
              " 'avez',\n",
              " 'aviez',\n",
              " 'avions',\n",
              " 'avons',\n",
              " 'ayant',\n",
              " 'ayante',\n",
              " 'ayantes',\n",
              " 'ayants',\n",
              " 'ayez',\n",
              " 'ayons',\n",
              " 'c',\n",
              " 'ce',\n",
              " 'ces',\n",
              " 'd',\n",
              " 'dans',\n",
              " 'de',\n",
              " 'des',\n",
              " 'du',\n",
              " 'elle',\n",
              " 'en',\n",
              " 'es',\n",
              " 'est',\n",
              " 'et',\n",
              " 'eu',\n",
              " 'eue',\n",
              " 'eues',\n",
              " 'eurent',\n",
              " 'eus',\n",
              " 'eusse',\n",
              " 'eussent',\n",
              " 'eusses',\n",
              " 'eussiez',\n",
              " 'eussions',\n",
              " 'eut',\n",
              " 'eux',\n",
              " 'eûmes',\n",
              " 'eût',\n",
              " 'eûtes',\n",
              " 'furent',\n",
              " 'fus',\n",
              " 'fusse',\n",
              " 'fussent',\n",
              " 'fusses',\n",
              " 'fussiez',\n",
              " 'fussions',\n",
              " 'fut',\n",
              " 'fûmes',\n",
              " 'fût',\n",
              " 'fûtes',\n",
              " 'il',\n",
              " 'ils',\n",
              " 'j',\n",
              " 'je',\n",
              " 'l',\n",
              " 'la',\n",
              " 'le',\n",
              " 'les',\n",
              " 'leur',\n",
              " 'lui',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'mais',\n",
              " 'me',\n",
              " 'mes',\n",
              " 'moi',\n",
              " 'mon',\n",
              " 'même',\n",
              " 'n',\n",
              " 'ne',\n",
              " 'nos',\n",
              " 'notre',\n",
              " 'nous',\n",
              " 'on',\n",
              " 'ont',\n",
              " 'ou',\n",
              " 'par',\n",
              " 'pas',\n",
              " 'pour',\n",
              " 'qu',\n",
              " 'que',\n",
              " 'qui',\n",
              " 's',\n",
              " 'sa',\n",
              " 'se',\n",
              " 'sera',\n",
              " 'serai',\n",
              " 'seraient',\n",
              " 'serais',\n",
              " 'serait',\n",
              " 'seras',\n",
              " 'serez',\n",
              " 'seriez',\n",
              " 'serions',\n",
              " 'serons',\n",
              " 'seront',\n",
              " 'ses',\n",
              " 'soient',\n",
              " 'sois',\n",
              " 'soit',\n",
              " 'sommes',\n",
              " 'son',\n",
              " 'sont',\n",
              " 'soyez',\n",
              " 'soyons',\n",
              " 'suis',\n",
              " 'sur',\n",
              " 't',\n",
              " 'ta',\n",
              " 'te',\n",
              " 'tes',\n",
              " 'toi',\n",
              " 'ton',\n",
              " 'tu',\n",
              " 'un',\n",
              " 'une',\n",
              " 'vos',\n",
              " 'votre',\n",
              " 'vous',\n",
              " 'y',\n",
              " 'à',\n",
              " 'étaient',\n",
              " 'étais',\n",
              " 'était',\n",
              " 'étant',\n",
              " 'étante',\n",
              " 'étantes',\n",
              " 'étants',\n",
              " 'étiez',\n",
              " 'étions',\n",
              " 'été',\n",
              " 'étée',\n",
              " 'étées',\n",
              " 'étés',\n",
              " 'êtes'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pour restreindre le conteun de la phrase tous les mots qui figurent dans cette list\n",
        "clean_words=[]\n",
        "for token in return_token(result[1]):\n",
        "  if token not in stopWordss:\n",
        "    clean_words.append(token)\n",
        "    clean_words"
      ],
      "metadata": {
        "id": "SYGAbqMUj2EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streminig"
      ],
      "metadata": {
        "id": "HGuO-ufxzARq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "x-cfjXSjlOrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stremmer =SnowballStemmer(language='french')\n",
        "def return_stem(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  return[stremmer.stem(x.text)for x in doc]"
      ],
      "metadata": {
        "id": "fDuIsXBhzhiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stem1 = return_stem(test)\n",
        "stem1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb2gUZ5e0Dpw",
        "outputId": "12de5f81-0a98-4dc2-e2aa-117447cf6c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " 'il',\n",
              " 'ya',\n",
              " 'eu',\n",
              " 'une',\n",
              " 'coupur',\n",
              " 'de',\n",
              " 'réseau',\n",
              " 'à',\n",
              " 'marseil',\n",
              " '.',\n",
              " 'la',\n",
              " 'pann',\n",
              " 'a',\n",
              " 'effect',\n",
              " '300.00',\n",
              " 'utilis',\n",
              " ',']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reconnaissance d'entité nommée"
      ],
      "metadata": {
        "id": "UaTxpZeV1Tkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def return_MER(sentence):\n",
        "  # Tokeniser la phrase\n",
        "  doc = nlp(sentence)\n",
        "  #return le texte et le label pour chaque entité\n",
        "  return[(x.text, x.label_)for x in doc.ents]"
      ],
      "metadata": {
        "id": "AFP-v5tu0Yto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "return_MER('la sciège du STEG est à Souse')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWPCIUxO1F1B",
        "outputId": "59b2fedc-9a76-4207-f1a6-3d37b7146297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('STEG', 'ORG'), ('Souse', 'LOC')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# visualisation"
      ],
      "metadata": {
        "id": "Il0qUDNL2lk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "doc= nlp(\"le sciége du STEG est à sousse\")\n",
        "displacy.render(doc,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "QswmrCck1OYF",
        "outputId": "94c87e21-817e-4481-bcfb-f9cf2bde8ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">le sciége du \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    STEG\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " est à sousse</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spécifier des paraméter à displacy"
      ],
      "metadata": {
        "id": "0e7_oQqU3ZwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Le sciége du STEG est à sousse\")\n",
        "colors={\"ORG\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7 )\",\"LOC\":\"linear-gradient(90deg, #aa9cfc, #fc9ce7 )\"}\n",
        "options = {\"ents\": ['ORG','LOC'],\"colors\":colors}\n",
        "displacy.render(doc, style='ent', jupyter=True, options=options)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "mZZmFEjq3OkJ",
        "outputId": "954dd20e-3286-425b-910b-12c6251290a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Le sciége du \n",
              "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7 ); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    STEG\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " est à sousse</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JOGSrjyd4n4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}