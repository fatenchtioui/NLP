{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF84uvYeE86DkQrb4cpypj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatenchtioui/NLP/blob/main/nlp_tp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aD8w9HessIw"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntkJIE9zv5GZ",
        "outputId": "fe73498a-389a-4b93-ac69-a9eb18c2533e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def traiter(url):\n",
        "  # initilise deux listes vides pour stocker les pharses et les classes\n",
        "  pharses=[]\n",
        "  classes=[]\n",
        "  #ouvre le fichier en utilisent l'URL fournie en mode lecture ('r)et en spécifient l'encodage UTF-8\n",
        "  with open (url, mode='r', encoding='utf-8')as f:\n",
        "  #parcours chaque ligne dans le fiche\n",
        "      for ligne in f:\n",
        "      #deivise la ligne en utiliseant le caractére de tabulation '\\t' comme délimiteur\n",
        "         parties = ligne.split('\\t')\n",
        "      #ajoute la quatriéme partir (indice 3) à la liste des pharase\n",
        "         pharses.append(parties[3])\n",
        "# convertit la deuxiéme partie (indice1) en entier et l'ajoute à la liste des classes\n",
        "         classes.append(int(parties[1]))\n",
        "# returne les listes des pharases et de classes\n",
        "  return  pharses, classes"
      ],
      "metadata": {
        "id": "WVlTOlnMvlfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = traiter('/content/drive/MyDrive/folder/in_domain_train.tsv')\n",
        "X_train[:3], Y_train[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp_4aRg-y-yH",
        "outputId": "15589632-b931-4e52-8606-495052e2f630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([\"our friends wo n't buy this analysis , let alone the next one we propose .\\n\",\n",
              "  \"one more pseudo generalization and i 'm giving up .\\n\",\n",
              "  \"one more pseudo generalization or i 'm giving up .\\n\"],\n",
              " [1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cNOpBl-3qPc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_dev, Y_dev = traiter('/content/drive/MyDrive/folder/in_domain_dev.tsv')\n",
        "X_dev[:3], Y_dev[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FowXmOz9zirb",
        "outputId": "09f5d33d-ed81-4edb-bedd-7ca105600ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['the sailors rode the breeze clear of the rocks .\\n',\n",
              "  'the weights made the rope stretch over the pulley .\\n',\n",
              "  'the mechanical doll wriggled itself loose .\\n'],\n",
              " [1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## création et utilisation d'un modéle de langage N_gramme basé sur la bibliothéqie NLTK"
      ],
      "metadata": {
        "id": "2SyzIKzV0c2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import MLE, Laplace, KneserNeyInterpolated\n",
        "from nltk.lm import Vocabulary\n",
        "from nltk.lm.preprocessing import pad_both_ends,padded_everygram_pipeline\n",
        "class NGramme:\n",
        "    #n= 2 la taille des N-grammes\n",
        "    #cutoff=2 --> tous les mots qui apparaissent moins de 2 fois dans les données\n",
        "    # d'apprentissage seront traités comme des mots rares ou inconnus\n",
        "    def __init__(self, n=2, cutoff=2):\n",
        "        self.n = n\n",
        "        # créer un modèle de langage Kneser-Ney Interpolated qui utilise des N-grammes\n",
        "        #de taille n et un vocabulaire qui gère les mots rares ou inconnus en fonction  de cutoff=2.\n",
        "        # Le modèle sera utilisé pour estimer les probabilités des séquences de mots dans le texte,\n",
        "        # et le vocabulaire configuré aidera à gérer les mots peu fréquents de manière efficace.\n",
        "        self.kn = KneserNeyInterpolated(n, vocabulary=Vocabulary(unk_cutoff=cutoff))\n",
        "\n",
        "    def preprocess(self, X):\n",
        "        # liste pour stocker les phrases prétraitées\n",
        "        sent_words = []\n",
        "        #pour chaque phrase\n",
        "        for sent in X:\n",
        "            # Divise chaque phrase en mots et ajoute un padding au début et à la fin pour les n-grams\n",
        "            sent_words.append(list(pad_both_ends(sent.split(), self.n)))\n",
        "            #sent.split(): diviser la phrase en mots (\\t)\n",
        "            #pad_both_ends(): ajouter un padding <s> </s> au début et à la fin de la liste de mots.\n",
        "        return sent_words\n",
        "\n",
        "    #Fonct pour entraîner le modèle de langage Kneser-Ney Interpolated sur les données textuelles prétraitées\n",
        "    def entrainer(self, X):\n",
        "        sent_words = self.preprocess(X)\n",
        "        #train = les N-grammes extraits des données d'apprentissage\n",
        "        #vocab = un objet de vocabulaire qui stocke les N-grammesuniques extraits du corpus et\n",
        "        #aide le modèle de langage à effectuer des calculs sur les données d'apprentissage\n",
        "        train, vocab = padded_everygram_pipeline(self.n, sent_words)\n",
        "        vocab_l = list(vocab)\n",
        "        train_l = []\n",
        "        for t in train:\n",
        "            # prépare les données pour l'entraînement du modèle ( transformer chaque n-gramm en liste)\n",
        "            train_l.append(list(t))\n",
        "        # Entraîne le modèle Kneser-Ney Interpolated sur les données\n",
        "        #Le modèle sera ajusté pour estimer les probabilités des N-grammes en fonction des données d'apprentissage.\n",
        "        self.kn.fit(train_l, vocab_l)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DEa0I6n20bOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def noter_pharse(self, phrase):\n",
        "  notes=[]# liste pour stocker les notes\n",
        "  print('notation de la phrase:',phrase)\n",
        "  # parcourir la phrase en considérant des sous-séquence de mots de la taille des N-gramme\n",
        "  for i in range(len(phrase)-self.n + 1):\n",
        "    notes.append(self.kn.score(phrase(i+self.n-1), phrase[i:i + self.n- 1]))\n",
        "    return notes\n",
        "    # scores pour tout le corpus\n",
        "    def noter(self, X):\n",
        "      notes = []\n",
        "      sent_Word = self.preprocess(X)\n",
        "      for phrase in sent_Word:\n",
        "        notes.append(self.noter_pharse(phrase))\n",
        "        return notes\n",
        "        ngram = NGramme(3)\n",
        "        ngram.entrainer(X_train)\n",
        "        ngram.noter(X_train[:3])\n"
      ],
      "metadata": {
        "id": "HuC2vsnN9B98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import LONG_BINGET\n",
        "from nltk.util import ngrams\n",
        "import random\n",
        "def generer_texte(modele, mopt_depart, longueur=5):\n",
        "  texte_genere=[mopt_depart]\n",
        "  for _ in range(longueur):\n",
        "    #les probabilité des mots suivante en utilisant le modéle\n",
        "    prob_max =-float(\"inf\")\n",
        "    # initialisation la variantes prob_max pour une valeur négative infinie\n",
        "    mot_suivant=\"\"\n",
        "    phrase = texte_genere[-(modele.n-1):]\n",
        "    #initialise la varable phrase en extrayant le N-1\n",
        "    for mot in modele.kn.vocab:\n",
        "      print(\"-------------------\")\n",
        "      print(mot)\n",
        "      phrase.append(mot)\n",
        "      prob = modele.kn.score(mot, phrase[:1])\n",
        "      print(\"----------------\")\n",
        "      print(prob)\n",
        "      if prob > prob_max and mot != \"</s>\":\n",
        "        prob_max = prob\n",
        "        mot_suivant = mot\n",
        "        texte_genere.append(mot_suivant)\n",
        "        if  mot_suivant == \"</s>\":\n",
        "          break\n",
        "          texte_genere = \"\" .join(texte_genere)\n",
        "          return texte_genere\n",
        "          mopt_depart = \"our\"\n",
        "          texte_genere = generer_texte(ngram, mopt_depart, longueur=3)\n",
        "          print(texte_genere)\n",
        "\n"
      ],
      "metadata": {
        "id": "X_raSIN1EKPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6IsjrTc8KGck"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}